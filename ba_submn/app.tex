\pdfminorversion=4
\documentclass[12pt]{article}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,filecolor=blue,backref=page]{hyperref}
\usepackage{graphicx}
\usepackage{mathtools}

%

% For theorems and so on:
\usepackage[english]{babel}
\makeatletter
\def\thmhead@plain#1#2#3{%
  \thmname{#1}\thmnumber{\@ifnotempty{#1}{ }\@upn{#2}}%
  \thmnote{ {\the\thm@notefont#3}}}
\let\thmhead\thmhead@plain
\makeatother

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{result}{Result}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{assumption}{Assumption}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{example}{Example}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{note}{Note}
%\usepackage{apacite}

% New commands and operators:
\def\sigsqeps{\sigma^2_{\epsilon}}
\def\aeps{a_{\epsilon}}
\def\Asqeps{A_{\epsilon}^2}
\def\Sigmanu{\bSigma_{\nu}}
\def\munu{\bmu_{\nu}}
\def\sigsqmu{\sigma^2_{\mu}}
\def\amu{a_{\mu}}
\def\Asqmu{A_{\mu}^2}
\def\const{\text{const.}}
\def\rest{\text{rest}}
\def\mumu{\bmu_\mu}
\def\betamu{\bbeta_\mu}
\def\umu{\bu_\mu}
\def\numu{\bnu_\mu}
\def\Vpsi{\bV_\psi}
\newcommand{\sigsqb}[1]{\sigma^2_{b_{#1}}}
\newcommand{\ab}[1]{a_{b_{#1}}}
\newcommand\betapsi[1]{\bbeta_{\psi_{#1}}}
\newcommand\upsi[1]{\bu_{\psi_{#1}}}
\newcommand\nupsi[1]{\bnu_{\psi_{#1}}}
\newcommand\sigsqpsi[1]{\sigma^2_{\psi_{#1}}}
\newcommand\apsi[1]{a_{\psi_{#1}}}
\newcommand\Asqpsi[1]{A_{\psi_{#1}}^2}
\newcommand\hmu[1]{h_{\mu, #1}}
\newcommand\hmupsi[1]{\bh_{\mu \psi, #1}}
\newcommand\hmupsiL[2]{\bh^{(#1)}_{\mu \psi, #2}}
\newcommand\Hpsi[1]{\bH_{\psi, #1}}
\newcommand\HpsiL[2]{\bH^{(#1)}_{\psi, #2}}
\newcommand\Cpsi[1]{\bC_{\psi, #1}}
\newcommand\CTpsi[1]{\bC^{\intercal}_{\psi, #1}}
\newcommand\CpsiL[2]{\bC^{(#1)}_{\psi, #2}}
\newcommand\CpsiTL[2]{\bC^{(#1) \intercal}_{\psi, #2}}
\newcommand\VpsiL[1]{\bV^{(#1)}_{\psi}}
\newcommand\VpsiTL[1]{\bV^{(#1) \intercal}_{\psi}}
\newcommand\mupsi[1]{\bmu_{\psi_#1}}
\newcommand\zetaL[2]{\zeta^{(#1)}_{#2}}
\newcommand\bzetaL[2]{\bzeta^{(#1)}_{#2}}
\newcommand\bzetaTL[2]{\bzeta^{(#1) \intercal}_{#2}}
\newcommand\zetaLstar[2]{\zeta^{* (#1)}_{#2}}
\newcommand\bzetaLstar[2]{\bzeta^{* (#1)}_{#2}}
\newcommand\psiL[2]{\psi^{(#1)}_{#2}}
\newcommand\psiLstar[2]{\psi^{* (#1)}_{#2}}
\newcommand\bpsiL[2]{\bpsi^{(#1)}_{#2}}
\newcommand\nupsiL[2]{\bnu^{(#1)}_{\psi_{#2}}}
\newcommand\nupsiTL[2]{\bnu^{(#1) \intercal}_{\psi_{#2}}}
\newcommand\sigsqpsiL[2]{\sigma^{(#1) 2}_{\psi_{#2}}}
\newcommand\apsiL[2]{a^{(#1)}_{\psi_{#2}}}
\newcommand\bPsiL[1]{\bPsi^{(#1)}}
\newcommand\bXiL[1]{\bXi^{(#1)}}
\newcommand\tni[1]{\text{terms not involving $#1$}}

% For creating figures, tables and drawing graphs:
\usepackage[font={small}]{caption}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{cleveref}
\usepackage[labelformat=simple]{subcaption}
\renewcommand\thesubfigure{(\alph{subfigure})}
\usepackage{tikz}
\usetikzlibrary{arrows,positioning,calc,fit}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{automata}
\usepackage{tkz-euclide}
\renewcommand{\arraystretch}{1} % Adjust value for vertical spacing in tables
\usepackage[bottom]{footmisc} % Keeps floats above footnotes
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{rotating}
\usepackage{tcolorbox}
\usepackage{pgffor}

% For referencing an external document:
\usepackage{xr}
\externaldocument{main_ba}

% For macros:
\usepackage{import}
\import{}{macros}

\title{Supplementary Material for Bayesian Functional Principal Components Analysis via Variational Message Passing}
\date{}
\author{}
%\author[1,2,3]{Tui H. Nolan \thanks{Corresponding author: tn352@cam.ac.uk}}
%\author[4]{Jeff Goldsmith}
%\author[1,5]{David Ruppert}
%\affil[1]{School of Operations Research and Information Engineering, Cornell University}
%\affil[2]{Medical Research Council Biostatistics Unit, The University of Cambridge}
%\affil[3]{School of Mathematical and Physical Sciences, University of Technology Sydney}
%\affil[4]{Department of Biostatistics, Mailman School of Public Health, Columbia University}
%\affil[5]{Department of Statistics and Data Science, Cornell University}

%%% BEGIN DOCUMENT
\begin{document}

\maketitle

%%%%%%%%%%%%%%  APPENDICES  %%%%%%%%%%%%%%%

\appendix
\numberwithin{equation}{section}

%%%%%%%%%%%%%%  PROOF  OF  THEOREM  \ref{thm:orth_basis}  %%%%%%%%%%%%%%%

\section{Proof of Theorem \ref{thm:orth_basis}}
\label{app:proof_thm_orth_basis}

We first note that

\begin{equation}
	y_i (t) - \mu (t) = \sum_{l=1}^L z_{il} h_l (t), \quad i = 1, \dots, n.
\label{centered_kl_expansion}
\end{equation}

\noindent The existence of an orthonormal eigenbasis $\psi_1, \dots, \psi_L$ can be established via Gram-Schmidt
orthogonalization. We first set

\[
	\phi_1 \equiv h_1, \quad
	\phi_j \equiv h_j - \sum_{l=1}^{j-1} \frac{\langle \phi_l, h_j \rangle}{|| \phi_l ||^2} \phi_l, \quad j = 2, \dots, L.
\]

\noindent Next, set

\[
	\phi^*_j = \frac{\phi_j}{|| \phi_j ||}, \quad j = 1, \dots, L.
\]

\noindent Then $\phi^*_1, \dots, \phi^*_L$ form an orthonormal basis for the span of $h_1, \dots, h_L$. Therefore,
\eqref{centered_kl_expansion} can be re-written as

\[
	y_i (t) - \mu (t) = \sum_{l=1}^L \iota_{il} \phi^*_l (t), \quad i = 1, \dots, n,
\]

\noindent where

\[
	\iota_{il} \equiv z_{il} ||\phi_l|| + \sum_{j=l+1}^L z_{ij} \frac{\langle \phi_l , h_j \rangle}{||\phi_l||}, \quad
	l = 1, \dots, L-1, \quad
	\iota_{iL} \equiv z_{iL} ||\phi_L||.
\]

\noindent Note that $\iota_{i1}, \dots, \iota_{iL}$ are correlated.

Now, define $\biota_i \equiv \T{(\iota_{i1}, \dots, \iota_{iL})}$, $i = 1, \dots, n$. Since the curves $y_1, \dots, y_n$ are
random observations of a Gaussian process, we have

\[
	\biota_i \indsim \normal (0, \bSigma_\iota), \quad i = 1, \dots, n.
\]

\noindent Next, establish the eigendecomposition of $\Sigma_\iota$, such
that $\bSigma_\iota = \bQ_\iota \bLambda_\iota \T{\bQ_\iota}$, where $\bLambda_\iota$
is a diagonal matrix consisting of the eigenvalues of
$\bSigma_\iota$ in descending order, and the columns of $\bQ_\iota$
are the corresponding eigenvectors. Then, it can be easily seen that

\[
	\bzeta_i \equiv \T{\bQ_\iota} \biota_i \indsim \normal (0, \bLambda_\iota), \quad i = 1, \dots, n.
\]

\noindent That is, the elements of $\bzeta_i$ are uncorrelated and $\bzeta_1, \dots, \bzeta_n$ are independent.

Next, define the eigenvectors of $\bSigma_\iota$ as $\bq_1, \dots, \bq_L$, such that
$\bQ = [ \begin{array}{ccc} \bq_1 & \cdots & \bq_L \end{array} ]$.
Furthermore, define the elements of each of
the eigenvectors such that $\bq_l = \T{( q_{l1}, \dots, q_{lL} )}$, $l = 1, \dots, L$. Then, set

\[
	\psi_l \equiv \sum_{j=1}^L q_{jl} \phi^*_j, \quad l = 1, \dots, L.
\]

\noindent The orthonormality of $\psi_1, \dots, \psi_L$ is easily verified:

\begin{align*}
	\langle \psi_l , \psi_j \rangle
		&= \biggl\langle \sum_{m=1}^L q_{ml} \phi^*_m , \sum_{k=1}^L q_{kj} \phi^*_k \biggr\rangle
		= \sum_{m=1}^L \sum_{k=1}^L q_{ml} q_{kj} \langle \phi^*_m , \phi^*_k \rangle \\
		&= \sum_{m=1}^L q_{ml} q_{mj}
		= \T{\bq_l} \bq_j
		= \ind (l = j),
\end{align*}

\noindent where $\ind (\cdot)$ is the indicator function.

Finally, we have

\begin{align*}
	y_i (t) - \mu (t)
		&= \sum_{l=1}^L \iota_{il} \phi^*_l (t)
		= \sum_{l=1}^L \sum_{j=1}^L \zeta_{ij} q_{lj} \phi^*_l (t) \\
		&= \sum_{j=1}^L \zeta_{ij} \sum_{l=1}^L q_{lj} \phi^*_l (t)
		= \sum_{j=1}^L \zeta_{ij} \ \psi_j (t).
\end{align*}

\noindent This decomposition is unique up to a change of sign.

%%%%%%%%%%%%%%  EXPONENTIAL  FAMILY  FORM  %%%%%%%%%%%%%%%

\section{Exponential Family Form}
\label{app:exp_fam_form}

We first describe the exponential family form for the normal distribution.
For a $d \times 1$ multivariate normal
random vector $\bx \sim \normal (\bmu, \bSigma)$, the probability density function of $\bx$ can be shown to satisfy

\begin{equation}
	p(\bx) = \exp \left\{ \T{\bT_{\vect} (\bx)} \bdeta_{\vect} - A_{\vect} (\bdeta_{\vect}) - \frac{d}{2} \log (2 \pi) \right\},
\label{vec_repn}
\end{equation}

\noindent where $\bT_{\vect} (\bx) \equiv \T{\{ \T{\bx}, \T{\vect ( \bx \T{\bx} )} \}}$ is the vector of sufficient statistics
and $\bdeta_{\vect} \equiv \T{( \T{\bdeta_{\vect, 1}}, \T{\bdeta_{\vect, 2}} )} \equiv \T{[ \T{( \bSigma^{-1} \bmu )},
-\frac12 \T{\{ \vect (\bSigma^{-1}) \}} ]}$ is the natural parameter vector.
The function $A_{\vect} (\bdeta_{\vect}) =
-\frac14 \T{\bdeta}_{\vect, 1} \{ \vect^{-1} (\bdeta_{\vect, 2}) \}^{-1} \bdeta_{\vect, 1}
- \frac12 \log | -2 \vect^{-1} (\bdeta_{\vect, 2}) |$ is the log-partition function.
The inverse mapping of the natural parameter vector is \cite[equation~S.4]{wand17}

\begin{equation}
	\bmu = -\frac12 \left\{ \vect^{-1} (\bdeta_{\vect, 2}) \right\}^{-1} \bdeta_{\vect, 1} \quad
	\text{and} \quad
	\bSigma = -\frac12 \left\{ \vect^{-1} (\bdeta_{\vect, 2}) \right\}^{-1}.
\label{gauss_vec_comm_params}
\end{equation}

\noindent We will refer to the representation of the multivariate normal probability density function in
\eqref{vec_repn} as the \emph{vec-based representation}.

Alternatively, a more storage-economical
representation of the multivariate normal probability density function is the \emph{vech-based representation}:

\[
	p (\bx) = \exp \left\{ \T{\bT_{\vech} (\bx)} \bdeta_{\vech} - A_{\vech} (\bdeta_{\vech}) - \frac{d}{2} \log (2 \pi) \right\},
\]

\noindent where the vector of sufficient statistics, the natural parameter vector and the log-partition function are,
$\bT_{\vech} (\bx) \equiv \T{\{ \T{\bx}, \T{\vect ( \bx \T{\bx} )} \}}$,
$\bdeta_{\vech} \equiv \T{( \T{\bdeta_{\vech, 1}}, \T{\bdeta_{\vech, 2}} )} \equiv \T{[ \T{( \bSigma^{-1} \bmu )}, \allowbreak
-\frac12 \T{ \T{\bD_d} \{ \vect (\bSigma^{-1}) \}} ]}$ and
$A_{\vech} (\bdeta_{\vech}) = -\frac14 \T{\bdeta}_{\vech, 1} \{ \vect^{-1} (\bD_d^{+ \intercal} \bdeta_{\vech, 2}) \}^{-1}
\bdeta_{\vech, 1} - \frac12 \log | -2 \vect^{-1} (\bD_d^{+ \intercal} \bdeta_{\vech, 2}) |$, respectively.
The inverse mapping of the natural parameter vector under the vech-based representation is

\begin{equation}
	\bmu = -\frac12 \left\{ \vect^{-1} (\bD_d^{+ \intercal}\bdeta_{\vech, 2}) \right\}^{-1} \bdeta_{\vech, 1} \quad
	\text{and} \quad
	\bSigma = -\frac12 \left\{ \vect^{-1} (\bD_d^{+ \intercal}\bdeta_{\vech, 2}) \right\}^{-1}.
\label{gauss_vech_comm_params}
\end{equation}

The other major distribution within the exponential family that is pivotal for this article is the inverse-$\chi^2$ distribution.
A random variable $x$ has an inverse-$\chi^2$ distribution with shape parameter $\xi > 0$ and scale parameter
$\lambda > 0$ if the probability density function of $x$ is

\[
	p(x) = 
		\frac{(\lambda/2)^{\xi/2}}{\Gamma (\xi/2)}
		x^{-(\xi + 2)/2} \exp \left( -\frac{\lambda}{2 x} \right) \ind (x > 0),
\]

\noindent where the vector of sufficient statistics, the natural parameter vector and the log-partition function are
$\bT (x) \equiv \T{( \log (x), 1/x )}$, $\bdeta = \T{(\eta_1, \eta_2)} = \T{\{-\frac12 (\xi + 2), -\frac{\lambda}{2} \}}$
and $A(\bdeta) \equiv \log \{ \Gamma (\xi/2) \} - \frac{\xi}{2} \log (\lambda/2)$, respectively. Note that
$\Gamma (z) \equiv \int_0^\infty u^{z - 1} e^u du$ is the gamma function,
$\ind (\cdot)$ is the indicator function, $\zeta > 0$ is the scale parameter and $\lambda > 0$ is the shape parameter.
The inverse mapping of the natural parameter vector is
$\xi = -2 \eta_1 - 2$ and $\lambda = -2 \eta_2$.

%%%%%%%%%%%%%%  PROOF  OF  LEMMA  \ref{lem:response_est}  %%%%%%%%%%%%%%%

\section{Proof of Lemma \ref{lem:response_est}}
\label{app:proof_lem_response_est}

To prove \eqref{post_curve_est}, we first note that posterior curve estimates from the VMP algorithm satisfy

\begin{align}
\begin{split}
	\yhat_i (\bt_g)
		&= \bC_g \E_q (\numu) + \sum_{l=1}^L \E_q (\zeta_{il}) \bC_g \E_q (\nupsi{l}) \\
		&= \muhat (\bt_g) + \sum_{l=1}^L \E_q (\zeta_{il}) \E_q \{ \psi_l (\bt_g) \} \\
		&= \muhat (\bt_g) + \bPsi \E_q (\bzeta_i) \\
		&= \muhat (\bt_g) + \bU_\psi \bD_\psi \T{\bV_\psi} \E_q (\bzeta_i) \\
		&= \muhat (\bt_g) + \bU_\psi \bQ \bLambda^{1/2} \bLambda^{-1/2} \T{\bQ} \bD_\psi \T{\bV_\psi} \E_q (\bzeta_i) \\
		&= \muhat (\bt_g) + \overset{\bullet}{\bPsi} \overset{\bullet}{\bzeta}_i,
\end{split}
\label{E_q_y}
\end{align}

\noindent where $\overset{\bullet}{\bzeta}_i \equiv \T{(\overset{\textbf{.}}{\zeta}_{11}, \dots, \overset{\textbf{.}}{\zeta}_{1L})}$,
$i = 1, \dots, n$. Next, define

\[
	\bY \equiv \begin{bmatrix} \yhat_1 (\bt_g) & \dots & \yhat_n (\bt_g) \end{bmatrix}
\]

\noindent Then, \eqref{E_q_y} implies

\[
	\bY - \muhat (\bt_g) \T{\bone_N} = \overset{\bullet}{\bPsi} \T{\overset{\bullet}{\bXi}}.
\]

\noindent Now, let $\bc$ be the $L \times 1$ vector, with $|| \overset{\textbf{.}}{\psi}_l ||$ as the $l$th entry, $l = 1, \dots, L$.
Furthermore, let $1/\bc$ be the $L \times 1$ vector, with $1/|| \overset{\textbf{.}}{\psi}_l ||$ as the $l$th entry, $l = 1, \dots, L$.
Recall that we can approximate these values through numerical integration. Then,

\[
	\bY - \muhat (\bt_g) \T{\bone_N} = \overset{\bullet}{\bPsi} \diag (1/\bc) \diag (\bc) \T{\overset{\bullet}{\bXi}}.
\]

\noindent It is easy to see that this implies \eqref{post_curve_est}.

%%%%%%%%%%%%%%  PROOF  OF  PROPOSITION  \ref{prop:bi_orthogonal}  %%%%%%%%%%%%%%%

\section{Proof of Proposition \ref{prop:bi_orthogonal}}
\label{app:proof_prop_bi_eigenfunctions}

The independence of $\bzetahat_1, \dots, \bzetahat_n$ is a consequence of the independence assumption in
\eqref{fpca_mf_restrn}. Let $\bc$ and $1/\bc$ retain their definitions from Appendix
\ref{app:proof_lem_response_est}. Then, note that

\[
	\bzetahat_i
		= \diag (\bc) \overset{\textbf{.}}{\bzeta}_i
		= \diag (\bc) \bLambda^{-1/2} \T{\bQ} \bD_\psi \T{\bV_\psi} \E_q (\bzeta_i).
\]

\noindent Let $\bm_\zetahat$ be the sample mean vector of $\bzetahat_1, \dots, \bzetahat_n$. Then,

\begin{align*}
	\bzetahat_i - \bm_\zetahat
		&= \diag (\bc) \bLambda^{-1/2} \T{\bQ} \bD \T{\bV} \E_q (\bzeta_i) - \bm_\zetahat \\
		&= \diag (\bc) \bLambda^{-1/2} \T{\bQ} \{ \bD \T{\bV} \E_q (\bzeta_i) - \bQ \bLambda^{1/2} \diag (1/\bc) \bm_\zetahat \} \\
		&= \diag (\bc) \bLambda^{-1/2} \T{\bQ} \{ \bD \T{\bV} \E_q (\bzeta_i) - \bm_\zeta \}
\end{align*}

\noindent where $\bm_\zeta \equiv \bQ \bLambda^{1/2} \diag (1/\bc) \bm_\zetahat$ is the mean vector of
$\bD \T{\bV} \E_q (\bzeta_1), \dots, \bD \T{\bV} \E_q (\bzeta_n)$.
Then,

\begin{align*}
	\sum_{i=1}^n (\bzetahat_i - &\bm_\zetahat) \T{(\bzetahat_i - \bm_\zetahat)} \\
		&= \diag (\bc) \bLambda^{-1/2} \T{\bQ}
			\sum_{i=1}^n \left[
				\{ \bD_\psi \T{\bV_\psi} \E_q (\bzeta_i) - \bm_\zeta \}
					\T{\{ \bD_\psi \T{\bV_\psi} \E_q (\bzeta_i) - \bm_\zeta \}}
			\right]
			\bQ \bLambda^{-1/2} \diag (\bc) \\
		&= (n - 1) \diag (\bc) \bLambda^{-1/2} \T{\bQ} \bC_\zeta \bQ \bLambda^{-1/2} \diag (\bc) \\
		&= (n - 1) \diag (\bc) \bLambda^{-1/2} \T{\bQ} \bQ \bLambda \T{\bQ} \bQ \bLambda^{-1/2} \diag (\bc) \\
		&= (n - 1) \diag (\bc^2),
\end{align*}

\noindent which proves the results for the estimated scores.

Now we have

\[
	\sum_{i=1}^n \yhat_i (\bt_g)
		= \sum_{i=1}^n \left\{ \muhat (\bt_g) + \sum_{l=1}^L \zetahat_{il} \psihat_l (\bt_g) \right\}
		= \sum_{i=1}^n \left\{ \muhat (\bt_g) + \bPsihat \bzetahat_i \right\}
		= n \muhat (\bt_g) + n \bPsihat \bm_\zetahat,
\]

\noindent where $\bPsihat \equiv \bigl[ \begin{smallmatrix} \psihat_1 (\bt_g) & \cdots & \psihat_L (\bt_g)
\end{smallmatrix} \bigr]$.
Therefore, the sample covariance matrix of $\yhat_1 (\bt_g), \dots, \yhat_n (\bt_g)$ is such that

\begin{align*}
	\sum_{i=1}^n 
		&\left[ \yhat_i (\bt_g) - \muhat (\bt_g) - \bPsihat \bm_\zetahat \right]
			\T{\left[ \yhat_i (\bt_g) - \muhat (\bt_g) - \bPsihat \bm_\zetahat \right]} \\
		&= \sum_{i=1}^n \left( \bPsihat \bzetahat_i - \bPsihat \bm_\zetahat \right)
			\T{\left( \bPsihat \bzetahat_i - \bPsihat \bm_\zetahat \right)} \\
		&= \bPsihat \left\{ \sum_{i=1}^n (\bzetahat_i - \bm_\zetahat) \T{(\bzetahat_i - \bm_\zetahat)} \right\} \bPsihat^{\intercal} \\
		&= (n - 1) \bPsihat \diag (\bc^2) \bPsihat^{\intercal}.
\end{align*}

\noindent Simple rearrangement confirms that this is the eigenvalue decomposition of
the sample covariance matrix of $\yhat_1 (\bt_g), \dots, \yhat_n (\bt_g)$, proving the result for the vectors
$\psihat_1 (\bt_g), \dots, \psihat_L (\bt_g)$.

%%%%%%%%%%%%%%  DERIVATION  OF FPCA  GAUSSIAN  %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%  LIKELIHOOD  FRAGMENT                 %%%%%%%%%%%%%%%

\section{Derivation of the Functional Principal Component Gaussian Likelihood Fragment}
\label{app:fpca_gauss_lik_frag}

From \eqref{bayes_fpca_mod}, we have, for $i = 1, \dots, n$, 

\begin{equation}
	\log p (\by_i | \bnu, \bzeta_i, \sigsqeps) =
		-\frac{T_i}{2} \log (\sigsqeps)
		- \frac{1}{2\sigsqeps} \left|\left|
			\by_i - \bC_i \left( \numu + \sum_{l=1}^L \zeta_{il} \nupsi{l} \right)
		\right|\right|^2
		+ \const
\label{log_bayes_fpca_mod}
\end{equation}

First, we establish the natural parameter vector for each of the optimal posterior density functions. These natural
parameter vectors are essential for determining expectations with respect to the optimal posterior distribution.
From equation (10) of \citet{wand17}, we deduce that the natural parameter vector for $q (\bnu)$ is

\[
	\npq{\bnu} =
		\np{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bnu}
		+ \np{\bnu}{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)},
\]

\noindent the natural parameter vector for $q (\bzeta_i)$, $i = 1, \dots, n$, is

\[
	\npq{\bzeta_i} =
		\np{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bzeta_i}
		+ \np{\bzeta_i}{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)},
\]

\noindent and the natural parameter vector for $q(\sigsqeps)$ is

\[
	\npq{\sigsqeps} =
		\np{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\sigsqeps}
		+ \np{\sigsqeps}{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}.
\]

Next, we consider the updates for standard expectations that occur for each of
the random variables and random vectors in
\eqref{log_bayes_fpca_mod}. For $\bnu$, we need to determine the mean vector $\E_q (\bnu)$
and the covariance matrix $\Cov_q (\bnu)$. The expectations are taken with respect to the normalization
of

\[
	\msg{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bnu} (\bnu)
	\msg{\bnu}{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)} (\bnu),
\]

\noindent which is a multivariate normal density function with natural parameter vector $\npq{\bnu}$.
From \eqref{gauss_vec_comm_params}, we have

\begin{align}
\begin{split}
	\E_q (\bnu)
		&\longleftarrow
			-\frac12 \left[
				\vect^{-1} \left\{
					\left( \npq{\bnu} \right)_2
				\right\}
			\right]^{-1} \left( \npq{\bnu} \right)_1 \\
	\text{and} \quad
	\Cov_q (\bnu)
		&\longleftarrow
			-\frac12 \left[
				\vect^{-1} \left\{
					\left( \npq{\bnu} \right)_2
				\right\}
			\right]^{-1}.
\end{split}
\label{mom_lik_nu}
\end{align}

\noindent Furthermore, the mean vector has the form

\begin{equation}
	\E_q (\bnu) \equiv \T{
		\left\{ \T{\E_q (\numu)}, \T{\E_q (\nupsi{1})}, \dots, \T{\E_q (\nupsi{L})} \right\}
	},
\label{exp_lik_nu}
\end{equation}

\noindent and the covariance matrix has the form

\begin{equation}
	\Cov_q (\bnu) \equiv \begin{bmatrix}
		\Cov_q (\numu) & \Cov_q (\numu, \nupsi{1}) & \dots & \Cov_q (\numu, \nupsi{L}) \\
		\Cov_q (\nupsi{1}, \numu) & \Cov_q (\nupsi{1}) & \dots & \Cov_q (\nupsi{1}, \nupsi{L}) \\
		\vdots & \vdots & \ddots & \vdots \\
		\Cov_q (\nupsi{L}, \numu) & \Cov_q (\nupsi{L}, \nupsi{1}) & \dots & \Cov_q (\nupsi{L}) \\
	\end{bmatrix}.
\label{cov_lik_nu}
\end{equation}

\noindent Similarly, for each $i = 1, \dots, n$, we need to determine the optimal mean vector and covariance matrix
for $\bzeta_i$, which are $\E_q (\bzeta_i)$ and $\Cov_q (\bzeta_i)$, respectively. The expectations are taken with
respect to the normalization of

\[
	\msg{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bzeta_i} (\bzeta_i)
	\msg{\bzeta_i}{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)} (\bzeta_i),
\]

\noindent which is a multivariate normal density function with natural parameter vector $\npq{\bzeta_i}$.
According to \eqref{gauss_vech_comm_params},

\begin{align}
\begin{split}
	\E_q (\bzeta_i)
		&\longleftarrow
			-\frac12 \left[
				\vect^{-1} \left\{
					\bD_L^{+ \intercal}
					\left( \npq{\bzeta_i} \right)_2
				\right\}
			\right]^{-1} \left( \npq{\bzeta_i} \right)_1 \\
	\text{and} \quad
	\Cov_q (\bzeta_i)
		&\longleftarrow
			-\frac12 \left[
				\vect^{-1} \left\{
					\bD_L^{+ \intercal}
					\left( \npq{\bzeta_i} \right)_2
				\right\}
			\right]^{-1}, \quad \text{for $i = 1, \dots, n$.}
\end{split}
\label{exp_lik_zeta}
\end{align}

\noindent Finally, for $\sigsqeps$, we need to determine $\E_q (1/\sigsqeps)$, with the expectation taken with
respect to the normalization of

\[
	\msg{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\sigsqeps} (\sigsqeps)
	\msg{\sigsqeps}{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)} (\sigsqeps).
\]

\noindent This is an inverse-$\chi^2$ density function, with natural parameter vector $\npq{\sigsqeps}$.
According to Result 6 of \citet{maestrini20},

\[
	\E_q (1/\sigsqeps)
		\longleftarrow
			\frac{
				\left( \npq{\sigsqeps} \right)_1 + 1
			}{
				\left( \npq{\sigsqeps} \right)_2
			}.
\]

Now, we turn our attention to the derivation of the message passed from $p (\by | \bnu, \bzeta_1, \dots, \bzeta_n,
\sigsqeps)$ to $\bnu$. Notice that

\begin{equation}
	\bC_i \left( \numu - \sum_{l=1}^L \zeta_{il} \nupsi{l} \right) = (\T{\bzetatilde_i} \otimes \bC_i) \bnu.
\label{theta_simplification}
\end{equation}

\noindent Therefore, as a function of $\bnu$, \eqref{log_bayes_fpca_mod} can be re-written as

\begin{align*}
	\log p (\by_i | \bnu, \bzeta_i, \sigsqeps)
		& = -\frac{1}{2\sigsqeps} \left|\left|
			\by_i - (\T{\bzetatilde_i} \otimes \bC_i) \bnu
		\right|\right|^2 + \tni{\bnu} \\
		& = \T{\begin{bmatrix}
			\bnu \\
			\vect (\bnu \T{\bnu})
		\end{bmatrix}} \begin{bmatrix}
			\frac{1}{\sigsqeps} \T{(\T{\bzetatilde_i} \otimes \bC_i)} \by_i \\
			-\frac{1}{2\sigsqeps} \vect \left\{
				(\bzetatilde_i \T{\bzetatilde_i}) \otimes (\T{\bC_i} \bC_i)
			\right\}
		\end{bmatrix} + \tni{\bnu}.
\end{align*}

\noindent According to equation (8) of \citet{wand17}, the message from the factor $p (\by | \bnu,
\bzeta_1, \dots, \bzeta_n, \sigsqeps)$ to $\bnu$ is

\[
	\msg{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bnu} (\bnu) \propto
		\exp \left\{
			\T{\begin{bmatrix}
				\bnu \\
				\vect (\bnu \T{\bnu})
			\end{bmatrix}}
			\np{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bnu}
		\right\},
\]

\noindent which is proportional to a
multivariate normal density function. The update for the message's natural parameter vector,
in \eqref{np_lik_nu}, is dependent upon
the mean vector and covariance matrix of $\bzetatilde_i$, which are

\begin{equation}
	\E_q (\zetatilde_i) = \T{\{ 1, \T{\E_q (\bzeta_i)} \}} \quad
	\text{and} \quad
	\Cov_q (\zetatilde_i) = \blockdiag \left\{ 0, \Cov_q (\bzeta_i) \right\}, \quad
	\text{for $i = 1, \dots, n$,}
\label{exp_lik_zeta_tilde}
\end{equation}

\noindent where $\E_q (\bzeta_i)$ and $\Cov_q (\bzeta_i)$ are defined in \eqref{exp_lik_zeta}. Note that
a standard statistical result allows us to write

\begin{equation}
	\E_q (\bzetatilde_i \T{\bzetatilde_i}) =
		\Cov_q (\bzetatilde_i) + \E_q (\bzetatilde_i) \T{\E_q (\bzetatilde_i)}, \quad \text{for $i = 1, \dots, n$.}
\label{E_q_outer_zeta}
\end{equation}

Next, notice that

\begin{equation}
	\sum_{l=1}^L \zeta_{il} \nupsi{l} = \Vpsi \bzeta_i
\label{zeta_simplification}
\end{equation}

\noindent Then, for each $i = 1, \dots, n$, the log-density function
in \eqref{log_bayes_fpca_mod} can be represented as a function of $\bzeta_i$ by

\begin{align*}
	\log p (\by_i | \bnu, \bzeta_i, \sigsqeps)
		& = -\frac{1}{2\sigsqeps} \left|\left|
			\by_i - \bC_i \numu - \bC_i \Vpsi \bzeta_i
		\right|\right|^2 + \tni{\bzeta_i} \\
		& = \T{
			\begin{bmatrix}
				\bzeta_i \\
				\vech (\bzeta_i \T{\bzeta_i})
			\end{bmatrix}
		} \begin{bmatrix}
			\frac{1}{\sigsqeps} (\T{\Vpsi} \T{\bC_i} \by_i - \hmupsi{i}) \\
			-\frac{1}{2 \sigsqeps} \T{\bD_L} \vect (\Hpsi{i})
		\end{bmatrix} + \tni{\bzeta_i},
\end{align*}

\noindent According to equation (8) of \citet{wand17}, the message from the factor
$p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)$ to $\bzeta_i$ is 

\[
	\msg{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bzeta_i} (\bzeta_i) \propto
		\exp \left\{
			\T{\begin{bmatrix}
				\bzeta_i \\
				\vech (\bzeta_i \T{\bzeta_i})
			\end{bmatrix}}
			\np{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bzeta_i}
		\right\},
\]

\noindent which
is proportional to a multivariate normal density function. The message's natural parameter vector update, in
\eqref{np_lik_zeta}, is dependant on the following expectations that are yet to be determined:

\[
	\E_q (\Vpsi) \quad \text{and} \quad \E_q (\Hpsi{i}), \quad \E_q (\hmupsi{i}), \quad i = 1, \dots, n.
\]

\noindent Now, we have,

\begin{equation}
	\E_q (\Vpsi) = \begin{bmatrix}
		\E_q (\nupsi{1}) & \dots & \E_q (\nupsi{L})
	\end{bmatrix},
\label{E_q_Vpsi}
\end{equation}

\noindent where, for $l = 1, \dots, L$, $\E_q (\nupsi{l})$ is defined by \eqref{mom_lik_nu} and \eqref{exp_lik_nu}.
Next, $\E_q (\hmupsi{i})$ is an $L \times 1$ vector, with $l$th component being

\begin{equation}
	\E_q (\hmupsi{i})_l =
		\tr \{ \Cov_q (\numu, \nupsi{l}) \T{\bC_i} \bC_i \}
		+ \T{\E_q (\nupsi{l})} \T{\bC_i} \bC_i \E_q (\numu), \quad
	l = 1, \dots, L,
\label{exp_lik_hmupsi}
\end{equation}

\noindent which depends on sub-vectors of $\E_q (\bnu)$ and sub-blocks of $\Cov_q (\bnu)$ that are defined
in \eqref{exp_lik_nu} and \eqref{cov_lik_nu}, respectively. Finally, $\E_q (\Hpsi{i})$ is an $L \times L$ matrix,
with $(l, l')$ component being

\begin{equation}
	\E_q (\Hpsi{i})_{l, l'} =
		\tr \{ \Cov_q (\nupsi{l'}, \nupsi{l}) \T{\bC_i} \bC_i \}
		+ \T{\E_q (\nupsi{l})} \T{\bC_i} \bC_i \E_q (\nupsi{l'}), \quad
	l, l' = 1, \dots, L.
\label{exp_lik_Hpsi}
\end{equation}

The final message to consider is the message from $p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)$ to
$\sigsqeps$. As a function of $\sigsqeps$, \eqref{log_bayes_fpca_mod} takes the form

\begin{align*}
	\log p (\by_i | \bnu, \bzeta_i, \sigsqeps)
		& = -\frac{T_i}{2} \log (\sigsqeps) - \frac{1}{2 \sigsqeps} \left|\left|
			\by_i - \bC_i \bV \bzetatilde_i
		\right|\right|^2 + \tni{\sigsqeps} \\
		& = \T{
			\begin{bmatrix}
				\log (\sigsqeps) \\
				\frac{1}{\sigsqeps}
			\end{bmatrix}
		} \begin{bmatrix}
			-\frac{T_i}{2} \\
			-\frac12 \left|\left| \by_i - \bC_i \bV \bzetatilde_i \right|\right|^2
		\end{bmatrix} + \tni{\sigsqeps},
\end{align*}

\noindent According to equation (8) of \citet{wand17}, the message from $p (\by | \bnu,
\bzeta_1, \dots, \bzeta_n, \sigsqeps)$ to $\sigsqeps$ is

\[
	\msg{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\sigsqeps} (\sigsqeps) \propto
		\exp \left\{
			\T{\begin{bmatrix}
				\log (\sigsqeps) \\
				1/\sigsqeps
			\end{bmatrix}}
			\np{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\sigsqeps}
		\right\},
\]

\noindent which is proportional
to an inverse-$\chi^2$ density function. The message's natural parameter vector, in \eqref{np_lik_sigsqeps}, depends
on the mean of the square norm $|| \by_i - \bC_i \bV \bzetatilde_i ||^2$, for $i = 1, \dots, n$.
This expectation takes the form

\begin{align*}
	\E_q \left(
		\left|\left| \by_i - \bC_i \bV \bzetatilde_i \right|\right|^2
	\right) =
		& \T{\by_i} \by_i - 2 \T{\E_q (\bzetatilde_i)} \T{\E_q (\bV)} \T{\bC_i} \by_i \\
		& + \tr \left[
			\left\{ \Cov_q (\bzetatilde_i) + \E_q (\bzetatilde_i) \T{\E_q (\bzetatilde_i)} \right\} \E_q (\bH_i)
		\right],
\end{align*}

\noindent where we introduce the matrices

\begin{equation}
	\bH_i \equiv \begin{bmatrix}
		\hmu{i} & \T{\hmupsi{i}} \\
		\hmupsi{i} & \Hpsi{i}
	\end{bmatrix}, \quad
	\text{for $i = 1, \dots, n$},
\label{H_mat}
\end{equation}

\noindent and vectors

\begin{equation}
	\hmu{i} \equiv \T{\numu} \T{\bC_i} \bC_i \numu, \quad
	\text{for $i = 1, \dots, n$}.
\label{hmu_vec}
\end{equation}

\noindent For each $i = 1, \dots, n$, the mean vector $\E_q (\bzetatilde_i)$ and $\Cov_q (\bzetatilde_i)$ are defined in
\eqref{exp_lik_zeta_tilde}. However, $\E_q (\bV)$ and $\E_q (\bH_i)$, $i = 1, \dots, n$, are yet to be determined.
We then have,

\[
	\E_q (\bV) = \begin{bmatrix}
		\E_q (\numu) & \E_q (\nupsi{1}) & \dots & \E_q (\nupsi{L})
	\end{bmatrix},
\]

\noindent where the component mean vectors are defined by \eqref{exp_lik_nu}.
For each $i = 1, \dots, n$, the expectation of $\bH_i$, defined in \eqref{H_mat},
with respect to the optimal posterior distribution is

\[
	\E_q (\bH_i) \equiv \begin{bmatrix}
		\E_q (\hmu{i}) & \T{\E_q (\hmupsi{i})} \\
		\E_q (\hmupsi{i}) & \E_q (\Hpsi{i})
	\end{bmatrix},
\]

\noindent where $\hmu{i}$ is defined in \eqref{hmu_vec} with expected value

\[
	\E_q (\hmu{i}) \equiv
		\tr \{ \Cov_q (\numu) \T{\bC_i} \bC_i \}
		+ \T{\E_q (\numu)} \T{\bC_i} \bC_i \E_q (\numu).
\]

\noindent Furthermore, $\E_q (\hmupsi{i})$ and $\E_q (\Hpsi{i})$ are defined in \eqref{exp_lik_hmupsi} and
\eqref{exp_lik_Hpsi}, respectively.

The FPCA Gaussian likelihood fragment, summarized in Algorithm \ref{alg:fpca_gauss_lik_frag}, is a
proceduralization of these results.

%%%%%%%%%%%%%%  DERIVATION  MEAN  AND  FPC                  %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%  GAUSSIAN  PENALIZATION  FRAGMENT  %%%%%%%%%%%%%%%

\section{Derivation of the Functional Principal Component Gaussian Penalization Fragment}
\label{app:mean_fpc_gauss_pen_frag}

From \eqref{bayes_fpca_mod}, we have, for $l = 1, \dots, L$,

\begin{align}
\begin{split}
	\log p (\numu, \nupsi{l} | \sigsqmu, \sigsqpsi{l}) =
		& -\frac{K}{2} \log (\sigsqmu) - \frac{K}{2} \log (\sigsqpsi{l})
			- \frac12 \T{(\betamu - \bmu_{\betamu})} \bSigma_{\betamu}^{-1} (\betamu - \bmu_{\betamu}) \\
		& - \frac{1}{2 \sigsqmu} \T{\umu} \umu
			- \frac12 \T{(\betapsi{l} - \bmu_{\betapsi{l}})} \bSigma_{\betapsi{l}}^{-1} (\betapsi{l} - \bmu_{\betapsi{l}})
			- \frac{1}{2 \sigsqpsi{l}} \T{\upsi{l}} \upsi{l}.
\end{split}
\label{log_mean_fpc_gauss_pen_factor}
\end{align}

First, we establish the natural parameter vector for each of the optimal posterior density functions. As explained
in Appendix \ref{app:fpca_gauss_lik_frag}, these natural
parameter vectors are essential for determining expectations with respect to the optimal posterior distribution.
According to equation (10) of \citet{wand17}, the natural parameter vector for $q (\bnu)$ is

\[
	\npq{\bnu} =
		\np{p(\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})}{\bnu}
		+ \np{\bnu}{p(\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})},
\]

\noindent the natural parameter vector for $q (\sigsqmu)$ is

\[
	\npq{\sigsqmu} =
		\np{p(\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})}{\sigsqmu}
		+ \np{\sigsqmu}{p(\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})},
\]

\noindent and, for $l = 1, \dots, L$, the natural parameter vector for $q(\sigsqpsi{l})$ is

\[
	\npq{\sigsqpsi{l}} =
		\np{p(\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})}{\sigsqpsi{l}}
		+ \np{\sigsqpsi{l}}{p(\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})}.
\]

Next, we consider the updates for standard expectations of each of the random variables and random vectors
that appear in \eqref{log_mean_fpc_gauss_pen_factor}. For $\bnu$, we require the mean vector $\E_q (\bnu)$
and covariance matrix $\Cov_q (\bnu)$ under the optimal posterior distribution. The expectations are taken with
respect to the normalization of

\[
	\msg{p(\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})}{\bnu} (\bnu)
	\msg{\bnu}{p(\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})} (\bnu),
\]

\noindent which is a multivariate normal density function with natural parameter vector $\npq{\bnu}$.
From \eqref{gauss_vec_comm_params}, we have

\begin{align}
\begin{split}
	\E_q (\bnu)
		&\longleftarrow
			-\frac12 \left[
				\vect^{-1} \left\{
					\left( \npq{\bnu} \right)_2
				\right\}
			\right]^{-1} \left( \npq{\bnu} \right)_1 \\
	\text{and} \quad
	\Cov_q (\bnu)
		&\longleftarrow
			-\frac12 \left[
				\vect^{-1} \left\{
					\left( \npq{\bnu} \right)_2
				\right\}
			\right]^{-1}.
\end{split}
\label{mom_pen_nu}
\end{align}

\noindent The sub-vectors and sub-matrices of $\E_q (\bnu)$ and $\Cov_q (\bnu)$ are identical to those in
\eqref{exp_lik_nu} and \eqref{cov_lik_nu}, respectively. For the functional principal components Gaussian penalization fragment,
however, we need to note further sub-vectors and sub-matrices. First,

\begin{equation}
	\E_q (\numu) \equiv \T{\left\{ \T{\E_q (\betamu)}, \T{\E_q (\umu)} \right\}} \quad
	\text{and} \quad
	\E_q (\nupsi{l}) \equiv \T{\left\{ \T{\E_q (\betapsi{l})}, \T{\E_q (\upsi{l})} \right\}}, \quad
	\text{for $l = 1, \dots, L$}
\label{exp_betau}
\end{equation}

\noindent and, second,

\begin{equation}
	\Cov_q (\numu) \equiv \begin{bmatrix}
		\Cov_q (\betamu) & \Cov_q (\betamu, \umu) \\
		\Cov_q (\umu, \betamu) & \Cov_q (\umu)
	\end{bmatrix}
\label{cov_betau_mu}
\end{equation}

\noindent and

\begin{equation}
	\Cov_q (\nupsi{l}) \equiv \begin{bmatrix}
		\Cov_q (\betapsi{l}) & \Cov_q (\betapsi{l}, \upsi{l}) \\
		\Cov_q (\upsi{l}, \betapsi{l}) & \Cov_q (\upsi{l})
	\end{bmatrix}, \quad
	\text{for $l = 1, \dots, L$.}
\label{cov_betau_psi}
\end{equation}

\noindent For $\sigsqmu$, we need to determine
$\E_q (1/\sigsqmu)$, with expectation taken with respect to the normalization of

\[
	\msg{p(\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})}{\sigsqmu} (\sigsqmu)
	\msg{\sigsqmu}{p(\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})} (\sigsqmu),
\]

\noindent which is an inverse-$\chi^2$ density function with natural parameter vector
$\npq{\sigsqmu}$. According to Result 6 of \citet{maestrini20},

\begin{equation}
	\E_q (1/\sigsqmu)
		\longleftarrow
			\frac{
				\left( \npq{\sigsqmu} \right)_1 + 1
			}{
				\left( \npq{\sigsqmu} \right)_2
			}.
\label{exp_pen_sigsqmu}
\end{equation}

\noindent Similar arguments can be used to show that

\begin{equation}
	\E_q (1/\sigsqpsi{l})
		\longleftarrow
			\frac{
				\left( \npq{\sigsqpsi{l}} \right)_1 + 1
			}{
				\left( \npq{\sigsqpsi{l}} \right)_2
			}, \quad \text{for $l = 1, \dots, L$.}
\label{exp_pen_sigsqpsi}
\end{equation}

Now, we turn our attention to the derivation of the messages passed from the factor. As a function of $\bnu$,
\eqref{log_mean_fpc_gauss_pen_factor} this can be re-written as

\begin{align*}
	\log p(\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})
		& = -\frac12 \T{\bnu} \Sigmanu^{-1} \bnu + \T{\bnu} \Sigmanu^{-1} \munu + \tni{\bnu} \\
		& = \T{\begin{bmatrix}
			\bnu \\
			\vect (\bnu \T{\bnu})
		\end{bmatrix}} \begin{bmatrix}
			\Sigmanu^{-1} \munu \\
			-\frac12 \vect (\Sigmanu^{-1})
		\end{bmatrix} + \tni{\bnu},
\end{align*}

\noindent According to equation (8) of \citet{wand17}, the message from the factor $p(\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})$
to $\bnu$ is

\[
	\msg{p (\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})}{\bnu} (\bnu)
		\propto
			\exp \left\{
				\T{\begin{bmatrix}
					\bnu \\
					\vect (\bnu \T{\bnu})
				\end{bmatrix}}
				\np{p (\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})}{\bnu}
			\right\},
\]

\noindent which is proportional to a multivariate normal density function.
The update for the message's natural parameter vector, in \eqref{np_pen_nu},
is dependant upon the expectation of $\Sigmanu^{-1}$, which is given by

\[
	\E_q (\Sigmanu^{-1}) =
		\blockdiag \left\{
			\begin{bmatrix}
				\bSigma_{\betamu} & \T{\textbf{O}} \\
				\textbf{O} & \E_q (1/\sigsqmu) \bI_K
			\end{bmatrix},
			\blockdiag_{l = 1, \dots, L} \left(
				\begin{bmatrix}
					\bSigma_{\betapsi{l}} & \T{\textbf{O}} \\
					\textbf{O} & \E_q (1/\sigsqpsi{l}) \bI_K
				\end{bmatrix}
			\right)
		\right\},
\]

\noindent where $\E_q (1/\sigsqmu)$ and, for $l = 1, \dots, L$, $\E_q (1/\sigsqpsi{l})$ are defined in
\eqref{exp_pen_sigsqmu} and \eqref{exp_pen_sigsqpsi}, respectively.

As a function of $\sigsqmu$, \eqref{log_mean_fpc_gauss_pen_factor} can be re-written as

\begin{align*}
	\log p(\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})
		& = -\frac{K}{2} \log (\sigsqmu) - \frac{1}{2\sigsqmu} \T{\umu} \umu + \tni{\sigsqmu} \\
		& = \T{\begin{bmatrix}
			\log (\sigsqmu) \\
			1/\sigsqmu
		\end{bmatrix}} \begin{bmatrix}
			-\frac{K}{2} \\
			-\frac12 \T{\umu} \umu
		\end{bmatrix} + \tni{\sigsqmu}.
\end{align*}

\noindent According to equation (8) of \citet{wand17}, the message from the factor $p(\bnu | \sigsqmu, \sigsqpsi{1}, \dots,
\sigsqpsi{L})$ to $\sigsqmu$ is

\[
	\msg{p (\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})}{\sigsqmu} (\sigsqmu)
		\propto
			\exp \left\{
				\T{\begin{bmatrix}
					\log (\sigsqmu) \\
					1/\sigsqmu
				\end{bmatrix}} 
				\np{p (\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})}{\sigsqmu}
			\right\},
\]

\noindent which is an inverse-$\chi^2$
density function upon normalization. The message's natural parameter vector update in \eqref{np_pen_sigsqmu}
depends on $\E_q (\T{\umu} \umu)$. Standard statistical results and sub-vector and sub-matrix definitions in
\eqref{exp_betau} and \eqref{cov_betau_mu} can be employed to show that

\[
	\E_q (\T{\umu} \umu) = \T{\E_q (\umu)} \E_q (\umu) + \tr \left\{ \Cov_q (\umu) \right\}.
\]

As a function of $\sigsqpsi{l}$, for $l = 1, \dots, L$, \eqref{log_mean_fpc_gauss_pen_factor} can be re-written as

\begin{align*}
	\log p(\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})
		& = -\frac{K}{2} \log (\sigsqpsi{l}) - \frac{1}{2\sigsqpsi{l}} \T{\upsi{l}} \upsi{l} + \tni{\sigsqpsi{l}} \\
		& = \T{\begin{bmatrix}
			\log (\sigsqpsi{l}) \\
			1/\sigsqpsi{l}
		\end{bmatrix}} \begin{bmatrix}
			-\frac{K}{2} \\
			-\frac12 \T{\upsi{l}} \upsi{l}
		\end{bmatrix} + \tni{\sigsqpsi{l}}.
\end{align*}

\noindent According to equation (8) of \citet{wand17}, the message from the factor $p(\bnu | \sigsqmu, \sigsqpsi{1}, \dots,
\sigsqpsi{L})$ to $\sigsqpsi{l}$ is

\[
	\msg{p (\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})}{\sigsqmu} (\sigsqpsi{l})
		\propto
			\exp \left\{
				\T{\begin{bmatrix}
					\log (\sigsqpsi{l}) \\
					1/\sigsqpsi{l}
				\end{bmatrix}} 
				\np{p (\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})}{\sigsqpsi{l}}
			\right\},
\]

\noindent which is an inverse-$\chi^2$
density function upon normalization. The message's natural parameter vector update in \eqref{np_pen_sigsqpsi}
depends on $\E_q (\T{\upsi{l}} \upsi{l})$. Standard statistical results and sub-vector and sub-matrix definitions in
\eqref{exp_betau} and \eqref{cov_betau_psi} can be employed to show that

\[
	\E_q (\T{\upsi{l}} \upsi{l}) = \T{\E_q (\upsi{l})} \E_q (\upsi{l}) + \tr \left\{ \Cov_q (\upsi{l}) \right\}.
\]

The functional principal component Gaussian penalization fragment, summarized in Algorithm \ref{alg:mean_fpc_gauss_pen_frag},
is a proceduralization of these results.

%%%%%%%%%%%%%%  TWO-LEVEL  SPARSE  MATRIX  BACKGROUND  %%%%%%%%%%%%%%%

\section{Two-level Sparse Matrix Background}
\label{app:two_lev_sparse_mat}

\citet[Section~2]{nolan20} introduce two-level sparse matrix problems as a formal process for streamlining the
the computations for two-level longitudinal data analysis \citep{pinheiro00}. \citet[Section~4]{nolanmw20}
extend these concepts to variational message passing for two-level linear mixed models. Here, we give a brief
overview of these concepts.

A two-level sparse matrix problem is concerned with solving the linear system $\bA \bx = \ba$, where

\begin{equation}
	\bA \equiv \begin{bmatrix}
		\bA_{11} & \bA_{12, 1} & \bA_{12, 2} & \cdots & \bA_{12, m} \\
		\bA^\intercal_{12, 1} & \bA_{22, 1} & \bO & \cdots & \bO \\
		\bA^\intercal_{12, 2} & \bO & \bA_{22, 2} & \cdots & \bO \\
		\vdots & \vdots & \vdots& \ddots & \vdots \\
		\bA^\intercal_{12, m} & \bO & \bO & \cdots & \bA_{22, m}
	\end{bmatrix}, \quad
	\ba \equiv \begin{bmatrix}
		\ba_1 \\
		\ba_{2, 1} \\
		\vdots \\
		\ba_{2, m}
	\end{bmatrix} \quad
	\text{and} \quad
	\bx \equiv \begin{bmatrix}
		\bx_1 \\
		\bx_{2, 1} \\
		\vdots \\
		\bx_{2, m}
	\end{bmatrix}
\label{two_lev_sparse_mat}
\end{equation}

\noindent and obtaining
the submatrices of $\bA^{-1}$ corresponding to the non-zero sub-matrices of $\bA$:

\begin{equation}
	\bA^{-1} \equiv \begin{bmatrix}
		\bA^{11} & \bA^{12, 1} & \bA^{12, 2} & \cdots & \bA^{12, m} \\
		\bA^{12, 1 \intercal} & \bA^{22, 1} & \bigtimes & \cdots & \bigtimes \\
		\bA^{12, 2 \intercal} & \bigtimes & \bA^{22, 2} & \cdots & \bigtimes \\
		\vdots & \vdots & \vdots& \ddots & \vdots \\
		\bA^{12, m \intercal} & \bigtimes & \bigtimes & \cdots & \bA^{22, m}
	\end{bmatrix}.
\label{A_inv}
\end{equation}

\noindent The sub-blocks of $\bA^{-1}$ represented by $\bigtimes$ are not of interest because
they correspond to between-group covariances in multilevel models. On the other hand, the
sub-blocks of $\bA^{-1}$ that are in the same position as the non-zero sub-blocks of $\bA$
are required for obtaining standard errors of within-group fits.

Algorithm A.1 in the supplementary material of \citet{nolanmw20}, called
\textsc{SolveTwoLevelSparseMatrix}, outputs the partitioning of $\bx_1, \bA^{11}$ and
$\{ \bx_{2, j}, \bA^{12, j}, \bA^{22, j} \}_{j = 1, \dots, m}$ through streamlined
computations that do not require direct inversion of the two-level sparse matrix $\bA$.

%%%%%%%%%%%%%%  DERIVATION  OF  THE  MLFPCA            %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%  GAUSSIAN. LIKELIHOOD  FRAGMENT  %%%%%%%%%%%%%%%

\section{Derivation of the Multilevel Functional Principal Component Gaussian Likelihood Fragment}
\label{app:mlfpca_gauss_lik_frag}

From \eqref{bayes_mlfpca_mod}, we have, for $i = 1, \dots, n$ and $j = 1, \dots, m_i$, 

\begin{align}
\begin{split}
	\log\ &p (\by_{ij} | \bnu, \bzetaL{1}{i}, \bzetaL{2}{ij}, \sigsqeps) = \\
		&-\frac{T_{ij}}{2} \log (\sigsqeps)
		- \frac{1}{2\sigsqeps} \left|\left|
			\by_{ij} - \bC_{ij} \left(
				\numu
				+ \sum_{l=1}^{L_1} \zetaL{1}{il} \nupsiL{1}{l}
				+ \sum_{l=1}^{L_2} \zetaL{2}{ijl} \nupsiL{2}{l}
			\right)
		\right|\right|^2 \\
		&+ \const
\end{split}
\label{log_bayes_mlfpca_mod}
\end{align}

From equation (10) of \citet{wand17}, we deduce that the natural parameter vector for $q (\bnu)$ is

\[
	\npq{\bnu} =
		\np{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bnu}
		+ \np{\bnu}{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)},
\]

\noindent the natural parameter vector for $q (\bzeta_i)$, $i = 1, \dots, n$, is

\[
	\npq{\bzeta_i} =
		\np{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bzeta_i}
		+ \np{\bzeta_i}{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)},
\]

\noindent and the natural parameter vector for $q(\sigsqeps)$ is

\[
	\npq{\sigsqeps} =
		\np{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\sigsqeps}
		+ \np{\sigsqeps}{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}.
\]

Next, we consider the updates for standard expectations that occur for each of
the random variables and random vectors in
\eqref{log_bayes_mlfpca_mod}. For $\bnu$, we need to determine the mean vector $\E_q (\bnu)$
and the covariance matrix $\Cov_q (\bnu)$. The expectations are taken with respect to the normalization
of

\[
	\msg{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bnu} (\bnu)
	\msg{\bnu}{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)} (\bnu),
\]

\noindent which is a multivariate normal density function with natural parameter vector $\npq{\bnu}$.
From \eqref{gauss_vec_comm_params}, we have

\begin{align}
\begin{split}
	\E_q (\bnu)
		&\longleftarrow
			-\frac12 \left[
				\vect^{-1} \left\{
					\left( \npq{\bnu} \right)_2
				\right\}
			\right]^{-1} \left( \npq{\bnu} \right)_1 \\
	\text{and} \quad
	\Cov_q (\bnu)
		&\longleftarrow
			-\frac12 \left[
				\vect^{-1} \left\{
					\left( \npq{\bnu} \right)_2
				\right\}
			\right]^{-1}.
\end{split}
\label{mom_ml_lik_nu}
\end{align}

In order to compute the $q$-density parameters for $q (\bzeta_i)$, note that the inverse covariance matrix has a
two-level sparse structure as defined in \eqref{two_lev_sparse_mat}. Naive computation of this matrix via methods
similar to \eqref{exp_lik_zeta} requires inversion of this sparse two-level inverse covariance matrix. Instead, we
turn to Corollary \ref{cor:zeta}, which is a direct consequence of Theorem 1 of \citet{nolan20}, for streamlined
computations of these parameters.
The solutions presented in Corollary \ref{cor:zeta} can be computed through the function
\textsc{SolveTwoLevelSparseMatrix} of \citet[Algorithm~A.1]{nolanmw20}

\begin{corollary}
	%
	For each $i = 1, \dots, n$ and $j = 1, \dots, m_i$,
	The updates of the quantities $\E_q (\bzetaL{1}{i})$, $\E_q (\bzetaL{2}{ij})$, $\Cov_q{\bzetaL{1}{i}}$,
	$\Cov_q (\bzetaL{2}{ij})$ and $\Cov_q (\bzetaL{1}{i}, \bzetaL{2}{ij})$ with respect to the normalization of
	%
	\[
		\msg{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bzeta_i} (\bzeta_i) \
		\msg{\bzeta_i}{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)} (\bzeta_i),
	\]
	%
	are expressible as a two-level sparse matrix problem (see Appendix \ref{app:two_lev_sparse_mat}) with
	%
	\[
		\bA \equiv -2 \begin{bmatrix}
			\vect^{-1} ( \T{\bD_{L_1}} \bdeta_{1, 2})
				& \T{\left[ \frac12 \stack\limits_{j = 1, \dots, m_i} \left\{ \vect^{-1} \T{(\bdeta_{2, 3, j})} \right\} \right]} \\
			\frac12 \stack\limits_{j = 1, \dots, m_i} \left\{ \vect^{-1} \T{(\bdeta_{2, 3, j})} \right\}
				& \blockdiag\limits_{j = 1, \dots, m_i} \left\{ \vect^{-1} (\bD_{L_2}^{+ \intercal} \bdeta_{2, 2, j}) \right\}
		\end{bmatrix}
	\]
	%
	\noindent and
	%
	\[
		\ba \equiv \begin{bmatrix}
			\bdeta_{1, 1} \\
			\stack\limits_{j = 1, \dots, m_i} (\bdeta_{2, 1, j})
		\end{bmatrix}, \quad
		\text{where} \quad
		\begin{bmatrix}
			\bdeta_{1, 1} \ (L_1 \times 1) \\
			\bdeta_{1, 2} \ (\frac12 L_1 (L_1 + 1) \times 1) \\
			\stack\limits_{j = 1, \dots, m_i} \left( \begin{bmatrix}
				\bdeta_{2, 1, j} \ (L_2 \times 1) \\
				\bdeta_{2, 2, j} \ (\frac12 L_2 (L_2 + 1) \times 1) \\
				\bdeta_{2, 3, j} \ (L_1 L_2 \times 1)
			\end{bmatrix} \right)
		\end{bmatrix}
	\]
	%
	is the partitioning of $\npq{\bzeta_i}$ that defines $\bdeta_{1, 1}$, $\bdeta_{1, 2}$ and
	$\{ \bdeta_{2, 1, j}, \bdeta_{2, 2, j}, \bdeta_{2, 3, j} \}_{j = 1, \dots, m_i}$. The solutions, according to
	\eqref{two_lev_sparse_mat} and \eqref{A_inv}, are $\E_q (\bzetaL{1}{i}) = \bx_1$,
	$\Cov_q (\bzetaL{1}{i}) = \bA^{11}$, $\E_q (\bzetaL{2}{ij}) = \bx_{2, j}$, $\Cov_q (\bzetaL{2}{ij}) = \bA^{22, j}$
	and $\Cov_q (\bzetaL{1}{i}, \bzetaL{2}{ij}) = \bA^{12, j}$.
	%
\label{cor:zeta}
\end{corollary}

\begin{proof}
	%
	Note that
	%
	\begin{align*}
		q (\bzeta_i)
			&\propto \msg{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bzeta_i} (\bzeta_i) \
				\msg{\bzeta_i}{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)} (\bzeta_i) \\
			&= \exp \left\{
				\T{\begin{bmatrix}
					\bzetaL{1}{i} \\
					\vech (\bzetaL{1}{i} \bzetaTL{1}{i}) \\
					\stack\limits_{j = 1, \dots, m_i} \left\{ \begin{bmatrix}
						\bzetaL{2}{ij} \\
						\vech (\bzetaL{2}{ij} \bzetaTL{2}{ij}) \\
						\vect (\bzetaL{1}{i} \bzetaTL{2}{ij})
					\end{bmatrix} \right\}
				\end{bmatrix}} \npq{\bzeta_i}
			\right\} \\
			&= \exp \left(
				\T{\bzeta_i} \ba - \frac12 \T{\bzeta_i} \bA \bzeta_i
			\right).
	\end{align*}
	%
	\noindent Standard manipulations then lead to 
	%
	\[
		\E_q (\bzeta_i) = \bA^{-1} \ba \quad \text{and} \quad \Cov_q (\bzeta_i) = \bA^{-1}
	\]
	%
	The solutions then follow from extraction of the sub-vectors of $\bx = \bA^{-1} \ba$ and the important
	sub-blocks of $\bA^{-1}$ according to \eqref{A_inv}.
	%
\end{proof}

Finally, for $\sigsqeps$, we need to determine $\E_q (1/\sigsqeps)$, with the expectation taken with
respect to the normalization of

\[
	\msg{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\sigsqeps} (\sigsqeps)
	\msg{\sigsqeps}{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)} (\sigsqeps).
\]

\noindent This is an inverse-$\chi^2$ density function, with natural parameter vector $\npq{\sigsqeps}$.
According to Result 6 of \citet{maestrini20},

\[
	\E_q (1/\sigsqeps)
		\longleftarrow
			\frac{
				\left( \npq{\sigsqeps} \right)_1 + 1
			}{
				\left( \npq{\sigsqeps} \right)_2
			}.
\]

Now, we turn our attention to the derivation of the message passed from $p (\by | \bnu, \bzeta_1, \dots, \bzeta_n,
\sigsqeps)$ to $\bnu$. Notice that

\begin{equation}
	\bC_{ij} \left(
		\numu + \sum_{l=1}^{L_1} \zetaL{1}{il} \nupsiL{1}{l} + \sum_{l=1}^{L_2} \zetaL{2}{ijl} \nupsiL{2}{l}
	\right) = (\T{\bzetatilde_{ij}} \otimes \bC_{ij}) \bnu.
\label{ml_theta_simplification}
\end{equation}

\noindent Therefore, as a function of $\bnu$, \eqref{log_bayes_mlfpca_mod} can be re-written as

\begin{align*}
	\log \ &p (\by_{ij} | \bnu, \bzetaL{1}{i}, \bzetaL{2}{ij}, \sigsqeps) \\
		& = -\frac{1}{2\sigsqeps} \left|\left|
			\by_{ij} - (\T{\bzetatilde_{ij}} \otimes \bC_{ij}) \bnu
		\right|\right|^2 + \tni{\bnu} \\
		& = \T{\begin{bmatrix}
			\bnu \\
			\vect (\bnu \T{\bnu})
		\end{bmatrix}} \begin{bmatrix}
			\frac{1}{\sigsqeps} \T{(\T{\bzetatilde_{ij}} \otimes \bC_{ij})} \by_{ij} \\
			-\frac{1}{2\sigsqeps} \vect \left\{
				(\bzetatilde_i \T{\bzetatilde_i}) \otimes (\T{\bC_i} \bC_i)
			\right\}
		\end{bmatrix} + \tni{\bnu}.
\end{align*}

\noindent According to equation (8) of \citet{wand17}, the message from the factor $p (\by | \bnu,
\bzeta_1, \dots, \bzeta_n, \sigsqeps)$ to $\bnu$ is 

\[
	\msg{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bnu} (\bnu) \propto
		\exp \left\{
			\T{\begin{bmatrix}
				\bnu \\
				\vect (\bnu \T{\bnu})
			\end{bmatrix}}
			\np{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bnu}
		\right\},
\]


\noindent which is proportional to a
multivariate normal density function. The update for the message's natural parameter vector,
in \eqref{np_ml_lik_nu}, is dependent upon
the mean vector and covariance matrix of $\bzetatilde_i$, which are

\begin{equation}
	\E_q (\bzetatilde_{ij}) = \begin{bmatrix}
		1 \\
		\E_q (\bzetaL{1}{i}) \\
		\E_q (\bzetaL{2}{ij})
	\end{bmatrix} \quad
	\text{and} \quad
	\Cov_q (\bzetatilde_{ij}) = \begin{bmatrix}
		0 & \T{\bzero_{L_1}} & \T{\bzero_{L_2}} \\
		\bzero_{L_1} & \Cov_q (\bzetaL{1}{i}) & \Cov_q (\bzetaL{1}{i}, \bzetaL{2}{ij}) \\
		\bzero_{L_2} & \Cov_q (\bzetaL{2}{ij}, \bzetaL{1}{i}) & \Cov_q (\bzetaL{2}{ij})
	\end{bmatrix},
\label{exp_ml_lik_zeta_tilde}
\end{equation}

\noindent for $i = 1, \dots, n$ and $j = 1, \dots, m_i$. Note that
a standard statistical result allows us to write

\begin{equation}
	\E_q (\bzetatilde_{ij} \T{\bzetatilde_{ij}}) =
		\Cov_q (\bzetatilde_{ij}) + \E_q (\bzetatilde_{ij}) \T{\E_q (\bzetatilde_{ij})}.
\label{E_q_outer_zeta}
\end{equation}

Next, notice that

\begin{equation}
	\sum_{l=1}^{L_1} \zetaL{1}{il} \nupsiL{1}{l} = \VpsiL{1} \bzetaL{1}{i} \quad
	\text{and} \quad
	\sum_{l=1}^{L_2} \zetaL{2}{ijl} \nupsiL{2}{l} = \VpsiL{2} \bzetaL{2}{ij}.
\label{ml_zeta_simplification}
\end{equation}

\noindent Then, for each $i = 1, \dots, n$ and $j = 1, \dots, m_i$, the log-density function
in \eqref{log_bayes_mlfpca_mod} can be represented as a function of $\bzeta_i$ by

\begin{align*}
	\log \ &p (\by_i | \bnu, \bzeta_i, \sigsqeps) \\
		& = \sum_{j=1}^{m_i} \log p (\by_{ij} | \bnu, \bzetaL{1}{i}, \bzetaL{2}{ij}, \sigsqeps) \\
		& = -\frac{1}{2\sigsqeps} \sum_{j = 1}^{m_i} \left|\left|
			\by_{ij} - \bC_{ij} (\numu + \VpsiL{1} \bzetaL{1}{i} + \VpsiL{2} \bzetaL{2}{ij})
		\right|\right|^2 + \tni{\bzeta_i} \\
		& = \T{
			\begin{bmatrix}
				\bzetaL{1}{i} \\
				\vech (\bzetaL{1}{i} \bzetaTL{1}{i}) \\
			\stack\limits_{j = 1, \dots, m_i} \left\{ \begin{bmatrix}
				\bzetaL{2}{ij} \\
				\vech (\bzetaL{2}{ij} \bzetaTL{2}{ij}) \\
				\vect (\bzetaL{1}{i} \bzetaTL{2}{ij})
			\end{bmatrix} \right\}
			\end{bmatrix}
		} \begin{bmatrix}
			\frac{1}{\sigsqeps} \sum_{j=1}^{m_i} (\VpsiTL{1} \T{\bC_{ij}} \by_{ij} - \hmupsiL{1}{ij}) \\
			-\frac{1}{2 \sigsqeps} \T{\bD_{L_1}} \sum_{j=1}^{m_i} \vect (\HpsiL{1, 1}{ij}) \\
			\stack\limits_{j = 1, \dots, m_i} \left\{ \begin{bmatrix}
				\frac{1}{\sigsqeps} (\VpsiTL{2} \T{\bC_{ij}} \by_{ij} - \hmupsiL{2}{ij}) \\
				-\frac{1}{2 \sigsqeps} \T{\bD_{L_2}} \vect (\HpsiL{2, 2}{ij}) \\
				-\frac{1}{\sigsqeps} \vect (\HpsiL{1, 2}{ij})
			\end{bmatrix} \right\}
		\end{bmatrix} \\ & \qquad+ \tni{\bzeta_i},
\end{align*}

\noindent According to equation (8) of \citet{wand17}, the message from the factor
$p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)$ to $\bzeta_i$ is 

\[
	\msg{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bzeta_i} (\bzeta_i) \propto
		\exp \left\{
			\T{
				\begin{bmatrix}
					\bzetaL{1}{i} \\
					\vech (\bzetaL{1}{i} \bzetaTL{1}{i}) \\
				\stack\limits_{j = 1, \dots, m_i} \left\{ \begin{bmatrix}
					\bzetaL{2}{ij} \\
					\vech (\bzetaL{2}{ij} \bzetaTL{2}{ij}) \\
					\vect (\bzetaL{1}{i} \bzetaTL{2}{ij})
				\end{bmatrix} \right\}
				\end{bmatrix}
			}
			\np{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\bzeta_i}
		\right\},
\]

\noindent which
is proportional to a multivariate normal density function. The message's natural parameter vector update, in
\eqref{np_ml_lik_zeta}, is dependant on the following expectations that are yet to be determined:

\[
	\E_q (\VpsiL{r}), \quad \E_q (\hmupsiL{r}{ij}) \quad \text{and} \quad \E_q (\HpsiL{r, s}{ij}),
\]

\noindent for $r, s = 1, 2$, $i = 1, \dots n$ and $j = 1, \dots, m_i$. Now, we have,

\begin{equation}
	\E_q (\VpsiL{r}) = \begin{bmatrix}
		\E_q (\nupsiL{r}{1}) & \dots & \E_q (\nupsiL{r}{L_r})
	\end{bmatrix}.
\label{E_q_Vpsi_r}
\end{equation}

\noindent Next, $\E_q (\hmupsiL{r}{ij})$ is an $L_r \times 1$ vector, with $l$th component being

\begin{equation}
	\E_q (\hmupsiL{r}{ij})_l =
		\tr \{ \Cov_q (\numu, \nupsiL{r}{l}) \T{\bC_{ij}} \bC_{ij} \}
		+ \T{\E_q (\nupsiL{r}{l})} \T{\bC_{ij}} \bC_{ij} \E_q (\numu),
\label{exp_ml_lik_hmupsi}
\end{equation}

\noindent for $l = 1, \dots, L_r$. Finally, $\E_q (\HpsiL{r, s}{ij})$ is an $L_r \times L_s$ matrix,
with $(l, l')$ component being

\begin{equation}
	\E_q (\HpsiL{r, s}{ij})_{l, l'} =
		\tr \{ \Cov_q (\nupsiL{s}{l'}, \nupsiL{r}{l}) \T{\bC_{ij}} \bC_{ij} \}
		+ \T{\E_q (\nupsiL{r}{l})} \T{\bC_{ij}} \bC_{ij} \E_q (\nupsiL{s}{l'}),
\label{exp_ml_lik_Hpsi}
\end{equation}

\noindent for $l = 1, \dots, L_r$ and $l' = 1, \dots, L_s$.

The final message to consider is the message from $p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)$ to
$\sigsqeps$. As a function of $\sigsqeps$, \eqref{log_bayes_mlfpca_mod} takes the form

\begin{align*}
	\log p (\by_{ij} | \bnu, \bzetaL{1}{i}, \bzetaL{2}{ij}, \sigsqeps)
		& = -\frac{T_{ij}}{2} \log (\sigsqeps) - \frac{1}{2 \sigsqeps} \left|\left|
			\by_{ij} - \bC_{ij} ( \numu - \VpsiL{1} \bzetaL{1}{i} - \VpsiL{2} \bzetaL{2}{ij} )
		\right|\right|^2 \\ &\qquad + \tni{\sigsqeps} \\
		& = \T{
			\begin{bmatrix}
				\log (\sigsqeps) \\
				\frac{1}{\sigsqeps}
			\end{bmatrix}
		} \begin{bmatrix}
			-\frac{T_{ij}}{2} \\
			-\frac12 \left|\left| \by_{ij} - \bC_{ij} ( \numu - \VpsiL{1} \bzetaL{1}{i} - \VpsiL{2} \bzetaL{2}{ij} ) \right|\right|^2
		\end{bmatrix} \\ &\qquad + \tni{\sigsqeps}.
\end{align*}

\noindent According to equation (8) of \citet{wand17}, the message from $p (\by | \bnu,
\bzeta_1, \dots, \bzeta_n, \sigsqeps)$ to $\sigsqeps$ is

\[
	\msg{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\sigsqeps} (\sigsqeps) \propto
		\exp \left\{
			\T{\begin{bmatrix}
				\log (\sigsqeps) \\
				1/\sigsqeps
			\end{bmatrix}}
			\np{p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)}{\sigsqeps}
		\right\},
\]

\noindent which is proportional
to an inverse-$\chi^2$ density function. The message's natural parameter vector, in \eqref{np_ml_lik_sigsqeps}, depends
on the mean of the square norm $|| \by_{ij} - \bC_{ij} ( \numu - \VpsiL{1} \bzetaL{1}{i} - \VpsiL{2} \bzetaL{2}{ij} ) ||^2$,
for $i = 1, \dots, n$ and $j = 1, \dots, m_i$. Setting $\hmu{ij} \equiv \T{\numu} \T{\bC_{ij}} \bC_{ij} \numu$, with
$q$-expectation

\[
	\E_q (\hmu{ij}) =
		\tr \{ \Cov_q (\numu) \T{\bC_{ij}} \bC_{ij} \}
		+ \T{\E_q (\numu)} \T{\bC_{ij}} \bC_{ij} \E_q (\numu).
\]

\noindent Streamlined computations for this expectation are such that

\begin{align*}
	\E_q &\left\{
		\left|\left| \by_{ij} - \bC_{ij} ( \numu - \VpsiL{1} \bzetaL{1}{i} - \VpsiL{2} \bzetaL{2}{ij} ) \right|\right|^2
	\right\} = \\
		& \T{\by_{ij}} \by_{ij}
			- 2 \T{\E_q (\numu)} \T{\bC_{ij}} \by_{ij}
			-2 \E_q \T{(\bzetaL{1}{i})} \E_q \T{(\VpsiL{1})} \T{\bC_{ij}} \by_{ij} \\
		&-2 \E_q \T{(\bzetaL{2}{ij})} \E_q \T{(\VpsiL{2})} \T{\bC_{ij}} \by_{ij}
			+ \E_q (\hmu{ij})
			+ 2 \E_q \T{(\bzetaL{1}{i})} \E_q (\hmupsiL{1}{ij}) \\
		& + 2 \E_q \T{(\bzetaL{2}{ij})} \E_q (\hmupsiL{2}{ij})
			+ \tr \{ \E_q (\bzetaL{1}{i} \bzetaTL{1}{i}) \E_q (\HpsiL{1, 1}{ij}) \} \\
		& + 2 \tr \{ \E_q (\HpsiL{1, 2}{ij}) \E_q (\bzetaL{2}{ij} \bzetaTL{1}{i}) \}
			+ \tr \{ \E_q (\bzetaL{2}{ij} \bzetaTL{2}{ij}) \E_q (\HpsiL{2, 2}{ij}) \}.
\end{align*}

The FPCA Gaussian likelihood fragment, summarized in Algorithm \ref{alg:mlfpca_gauss_lik_frag}, is a
proceduralization of these results.

%%%%%%%%%%%%%%  CONVERGENCE  AND  ALGORITHMIC  UPDATES  %%%%%%%%%%%%%%%

\section{Convergence and Algorithmic Updates}
\label{app:conv_updates}

%%%%%%%%%%%%%% Convergence

\subsection{Convergence}
\label{app:convergence}

Variational Bayesian inference, and hence VMP,
is based on the notion of minimal Kullback-Leibler divergence to approximate a
posterior density function. For arbitrary density functions $p_1$
and $p_2$ on $\R^d$, the Kullback-Leibler divergence of $p_1$ from $p_2$ is

\[
	\DKL (p_1, p_2) \equiv \int {\R^d} \log \left\{ \frac{p_1 (\bx)}{p_2 (\bx)} \right\} p_1 (\bx) d\bx.
\]

\noindent Note that

\begin{equation}
	\DKL (p_1, p_2) \ge 0.
\label{nonneg_dkl}
\end{equation}

Consider a generic Bayesian model with observed data vector $\by$ and parameter vector $\btheta \in \Theta$,
where $\bTheta$ is a parameter space. We make the
assumption that $\by$ and $\btheta$ are continuous random variables with density functions $p(\by)$ and $p(\btheta)$.
For the case where some components are discrete, a similar treatment applies with summations replacing integrals.
Next, let $q (\btheta)$ represent an arbitrary density function over the parameter space $\Theta$. The essence of
variational Bayesian inference is to restrict $q$ to a class of density functions $\Qsc$ and use the optimal $q$-density
function, defined by

\begin{equation}
	q^* (\btheta) \equiv \argmin_{q \in \Qsc} \ \DKL \{ q(\btheta), p (\btheta | \by) \},
\label{vb_dkl}
\end{equation}

\noindent as an approximation to the true posterior density function $p (\btheta | \by)$.

Simple algebraic arguments \cite[Section~2.1]{ormerod10} show that the marginal log-likelihood satisfies:

\[
	\log p (\by) = \DKL \{ q(\btheta), p (\btheta | \by) \} + \log \underline{p} (\by; q),
\]

\noindent where

\[
	\underline{p} (\by; q)
		\equiv \exp \left[
			\int \log \left\{
				\frac{p (\by, \btheta)}{q (\btheta)}
			\right\} q (\btheta) d\btheta
		\right],
\]

\noindent implying that

\[
	\log \underline{p} (\by; q) = \E_q \{ \log p (\by, \btheta) \} - \E_q \{ \log q (\btheta) \}.
\]

\noindent From the non-negativity condition of \eqref{nonneg_dkl}, we have

\[
	\log \underline{p} (\by; q) \le \log p (\by)
\]

\noindent showing that $\log \underline{p} (\by; q)$
is a lower-bound on the marginal log-likelihood. This leads to an
equivalent form for the optimisation problem in \eqref{vb_dkl}:

\begin{equation}
	q^* (\btheta) \equiv \argmax_{q \in \Qsc} \{ \log \underline{p} (\by; q) \}.
\label{vb_elbo}
\end{equation}

\noindent This alternate expression has the advantage of representing the
optimal $q$-density function as maximising the lower-bound on the marginal log-likelihood \cite{rohde16}.
For the remainder
of this article, we will address variational Bayesian inference with \eqref{vb_elbo}, rather than \eqref{vb_dkl}.

%%%%%%%%%%%%%% VMP Algorithms

\subsection{VMP Algorithms}
\label{app:vmp_algs}

Once the functional forms of the messages have been determined, the VMP iteration loop has the following
generic steps:

\begin{itemize}
	%
	\item Choose a fragment.
	%
	\item Update the parameter vectors of the messages passed from the factor’s neighboring stochastic
	nodes to the factor.
	%
	\item Update the parameter vectors of the messages passed from the factor to its neighboring
	stochastic nodes.
	%
\end{itemize}

Pseudocode for the VMP algorithm for the Bayesian FPCA model in \eqref{bayes_fpca_mod} is provided
in Algorithm \ref{alg:vmp_alg}. Likewise, pseudocode for the VMP algorithm for the Bayesian MlFPCA
model in \eqref{bayes_mlfpca_mod} is provided in Algorithm \ref{alg:vmp_ml_alg}

\begin{algorithm}
	\caption{
		Generic VMP algorithm for the Gaussian response FPCA model \eqref{bayes_fpca_mod} with
		mean field restriction \eqref{fpca_mf_restrn}.
	}
	\label{alg:vmp_alg}
	\begin{algorithmic}[1]
		\Inputs All hyperparameters and observed data
		\Initialize All factor to stochastic node messages.
			\Comment{\citet[Section 2.5]{wand17}}
		\Updates
			\While{$\lpyq$ has not converged}
				\State Update all stochastic node to factor messages.
					\Comment{\citet[Section 2.5]{wand17}}
				\State Update the fragment for $p (\by | \bnu, \bzeta_1, \dots, \bzeta_n, \sigsqeps)$
					\Comment{Algorithm \ref{alg:fpca_gauss_lik_frag}}
				\State Update the fragment for $p (\sigsqeps | \aeps)$
					\Comment{\citet[Algorithm 2]{maestrini20}}
				\State Update the fragment for $p (\aeps)$
					\Comment{\citet[Algorithm 1]{maestrini20}}
				\State Update the fragment for $p (\bnu | \sigsqmu, \sigsqpsi{1}, \dots, \sigsqpsi{L})$
					\Comment{Algorithm \ref{alg:mean_fpc_gauss_pen_frag}}
				\For{$i = 1, \dots, n$}
					\State Update the fragment for $p (\bzeta_i)$
						\Comment{\citet[Section 4.1.1]{wand17}}
				\EndFor
				\State Update the fragment for $p (\sigsqmu | \amu)$
					\Comment{\citet[Algorithm 2]{maestrini20}}
				\State Update the fragment for $p (\amu)$
					\Comment{\citet[Algorithm 1]{maestrini20}}
				\For{$l = 1, \dots, L$}
					\State Update the fragment for $p (\sigsqpsi{l} | \apsi{l})$
						\Comment{\citet[Algorithm 2]{maestrini20}}
					\State Update the fragment for $p (\apsi{l})$
						\Comment{\citet[Algorithm 1]{maestrini20}}
				\EndFor
			\EndWhile
			\State Rotate, translate and re-scale $\bPsi$ and $\bXi$.
				\Comment{Section \ref{sec:biorthogonal}}
		\Outputs $\muhat (\bt_g)$, $\psihat_1 (\bt_g), \dots, \psihat_L (\bt_g)$ and $\bzetahat_1, \dots, \bzetahat_n$.
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{
		Generic VMP algorithm for the MlFPCA model \eqref{bayes_mlfpca_mod} with
		mean field restriction \eqref{mlfpca_mf_restrn}.
	}
	\label{alg:vmp_ml_alg}
	\begin{algorithmic}[1]
		\Inputs All hyperparameters and observed data
		\Initialize All factor to stochastic node messages.
			\Comment{\citet[Section 2.5]{wand17}}
		\Updates
			\While{$\lpyq$ has not converged}
				\State Update all stochastic node to factor messages.
					\Comment{\citet[Section 2.5]{wand17}}
				\State Update the fragment for $p (\by | \bnu, \bzetaL{1}{}, \bzetaL{2}, \sigsqeps)$
					\Comment{Algorithm \ref{alg:mlfpca_gauss_lik_frag}}
				\State Update the fragment for $p (\sigsqeps | \aeps)$
					\Comment{\citet[Algorithm 2]{maestrini20}}
				\State Update the fragment for $p (\aeps)$
					\Comment{\citet[Algorithm 1]{maestrini20}}
				\State Update the fragment for $p ( \bnu | \sigsqmu, \bsigma^{(1) 2}_\psi, \bsigma^{(2) 2}_\psi )$
					\Comment{Algorithm \ref{alg:mean_fpc_gauss_pen_frag}}
				\For{$i = 1, \dots, n$}
					\State Update the fragment for $p (\bzeta_i)$
						\Comment{\eqref{np_ml_gauss_prior}}
				\EndFor
				\State Update the fragment for $p (\sigsqmu | \amu)$
					\Comment{\citet[Algorithm 2]{maestrini20}}
				\State Update the fragment for $p (\amu)$
					\Comment{\citet[Algorithm 1]{maestrini20}}
				\For{$l = 1, \dots, L_1$}
					\State Update the fragment for $p (\sigsqpsiL{1}{l} | \apsiL{1}{l})$
						\Comment{\citet[Algorithm 2]{maestrini20}}
					\State Update the fragment for $p (\apsiL{1}{l})$
						\Comment{\citet[Algorithm 1]{maestrini20}}
				\EndFor
				\For{$l = 1, \dots, L_2$}
					\State Update the fragment for $p (\sigsqpsiL{2}{l} | \apsiL{2}{l})$
						\Comment{\citet[Algorithm 2]{maestrini20}}
					\State Update the fragment for $p (\apsiL{2}{l})$
						\Comment{\citet[Algorithm 1]{maestrini20}}
				\EndFor
			\EndWhile
			\State Rotate, translate and re-scale $\bPsiL{1}$ and $\bXiL{1}$.
				\Comment{Section \ref{sec:mult_biorthogonal}}
			\State Rotate, translate and re-scale $\bPsiL{2}$ and $\bXiL{2}$.
				\Comment{Section \ref{sec:mult_biorthogonal}}
		\Outputs $\muhat (\bt_g)$, $\{ \psihat^{(1)}_l (\bt_g) \}_{l = 1, \dots, L_1}$,
			$\{ \psihat^{(2)}_l (\bt_g) \}_{l = 1, \dots, L_2}$, $\{ \bzetahat^{(1)}_i \}_{i = 1, \dots, n}$
			and $\{ \bzetahat^{(2)}_{ij} \}_{j = 1, \dots, m_i; i = 1, \dots, n}$.
	\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%  BIBLIOGRAPHY  %%%%%%%%%%%%%%%

\bibliographystyle{ba}
\bibliography{bibliography}

\end{document}



